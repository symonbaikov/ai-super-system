Брат, ты прав по-царски: делаем так, чтобы не падало. Ниже — короткий, но рабочий чек-лист как построить «непадающий» парсер-контур. Это именно про профилактику падений + авто-самовосстановление (а не «пусть второй спасает первого»).
Анти-падение: что внедряем
Оркестрация процессов
Запускаем парсер как сервис с авто-рестартом: systemd/PM2/Docker (restart: always).
2 экземпляра одного парсера (active/active) на двух разных хостах (или разных зонах/провайдерах).
Health-checks + watchdog
Эндпоинт /health и сердцебиение каждые 30 сек в мониторинг.
Если 3 пинга подряд нет → graceful restart самого воркера (а не всего узла).
Watchdog перезапускает конкретный браузер/сессию, если утечки/таймауты.
Очередь и бэкофф
Все задачи через Redis/BullMQ/Celery.
Экспоненциальные ретраи + circuit breaker (останавливаем поток на N секунд при шквале ошибок).
Идемпотентный sink (при повторной доставке событие не дублируется).
Дедуп и backpressure
Дедуп по tweet_id/url/normalized_text + 30m.
Если очереди растут — динамически понижаем частоту (авто-регулятор) и сужаем поиск до hot-мишеней.
Управление браузерами (X без API)
Пул Playwright/Puppeteer, рецикл сессий: перезапуск вкладки/процесса каждые N запросов.
Ротация User-Agent/прокси, спящие паузы с джиттером (anti-bot).
Храним session cookies зашифровано; авто-обновление; fallback-логин (2FA заранее настроен).
Прокси-пул и квоты
Несколько провайдеров прокси (ротация/бан-лист), контроль таймаутов.
Token-bucket rate-limit внутри парсера, чтобы никогда не выбивать лимиты.
Hot-режим безопасно
Базово 5 мин. При триггере включаем 1 мин только по hot_targets с TTL=25 мин и «охлаждением» (5 пустых циклов).
Наблюдаемость
Метрики: uptime, error-rate, latency, размер очередей, частота рестартов браузеров.
Логи с кореляцией по job_id, алерты в TG при: error-rate > 5% или ping-gap > 60 сек.
Blue-green / Canary
Обновления — через канареечный инстанс (1 воркер) → затем раскат на остальные.
Возможность мгновенно rollback.
Хранилище состояния
Redis (очередь/кеш) + Postgres (долговременные артефакты/конфиги).
Конфиги «на лету» (terms/handles/hot-правила) — без рестарта.
Безопасность
Webhook → HMAC подпись; IP-allowlist.
Таймауты и лимиты памяти на воркеры, чтобы утечки не валили весь узел.
План Б на случай внешних блокировок
Если X ужесточил анти-скрейп: автоматически сузить охват (только Tier-A/hot), поднять прокси-лимит, уменьшить параллелизм.
И иметь тонкий актор Apify как временный байпас на крит-термы (он не работает постоянно, а включается только при деградации).
Мини-шаблоны (быстро вставить и жить спокойно)
Docker compose (фрагмент):
services:
  parser:
    image: your/parser:latest
    restart: always
    environment:
      - WEBHOOK_URL=https://your.intake/api/golib/intake/dataimpulse
      - HMAC_SECRET=...
      - REDIS_URL=redis://...
      - PROXY_POOL_URL=...
      - HOT_TTL_MIN=25
      - TZ=Europe/Madrid
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 3s
      retries: 3
systemd (если без Docker):
[Unit]
Description=Twitter/TG Parser
After=network.target
[Service]
ExecStart=/usr/local/bin/parser
Restart=always
RestartSec=5
Environment=TZ=Europe/Madrid
LimitNOFILE=65535
[Install]
WantedBy=multi-user.target
Watchdog-логика (псевдокод):
if health_failures >= 3:
  restart(worker or browser)
if queue_size > HIGH_WATERMARK:
  reduce_concurrency()
  restrict_to_hot_targets()
Вывод
Мы не рассчитываем на падение — мы его профилактируем: изоляция воркеров, авто-рестарты, пул браузеров, прокси-ротация, backoff, health-мониторинг и канареечные выкладки. А тонкий Apify-актор — это не «костыль», а страховка на чёрный день, которую можно держать выключенной и включать только при внешней угрозе (жёсткие анти-боты/блоки).