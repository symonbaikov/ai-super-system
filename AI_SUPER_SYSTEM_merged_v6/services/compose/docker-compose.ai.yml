
version: "3.8"

volumes:
  hf_cache: {}
  models: {}

services:
  scout-llm:
    image: vllm/vllm-openai:latest
    ports: ["8001:8000"]
    environment:
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
    command: >
      --model ${SCOUT_MODEL:-Qwen/Qwen2-7B-Instruct}
      --dtype auto
      --max-model-len 4096
      --host 0.0.0.0
    volumes:
      - hf_cache:/root/.cache/huggingface
    restart: unless-stopped

  analyst-llm:
    image: vllm/vllm-openai:latest
    ports: ["8002:8000"]
    environment:
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
    command: >
      --model ${ANALYST_MODEL:-mistralai/Mixtral-8x7B-Instruct-v0.1}
      --dtype auto
      --max-model-len 8192
      --tensor-parallel-size ${ANALYST_TP:-1}
      --host 0.0.0.0
    volumes:
      - hf_cache:/root/.cache/huggingface
    restart: unless-stopped

  judge-llm:
    image: vllm/vllm-openai:latest
    ports: ["8003:8000"]
    environment:
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
    command: >
      --model ${JUDGE_MODEL:-mistralai/Mistral-7B-Instruct-v0.3}
      --dtype auto
      --max-model-len 4096
      --host 0.0.0.0
    volumes:
      - hf_cache:/root/.cache/huggingface
    restart: unless-stopped

  # Judge на CPU через llama.cpp (альтернатива judge-llm)
  judge-llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    command: >
      -m /models/judge-7b.Q5_K_M.gguf
      -c 4096
      -ngl 0
      -p 8004
      --host 0.0.0.0
    volumes:
      - models:/models
    ports:
      - "8004:8004"
    restart: unless-stopped

  # Reverse-proxy с ACL и Basic Auth
  gateway:
    image: nginx:1.25-alpine
    depends_on:
      - scout-llm
      - analyst-llm
      - judge-llm
      - judge-llamacpp
    ports:
      - "8080:8080"
    volumes:
      - ./gateway/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./gateway/.htpasswd:/etc/nginx/.htpasswd:ro
    restart: unless-stopped
